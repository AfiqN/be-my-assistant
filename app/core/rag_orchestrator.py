import logging
from typing import Optional, List, Tuple, Any # Any for model types

# Import functions and classes from other core modules
from .vector_store_manager import (
    query_vector_store
)
# Import LLM function from llm_interface
from .llm_interface import invoke_llm_langchain

from langchain_core.prompts import ChatPromptTemplate
from app.config import settings

# Access environment variables (still needed for API Key check)
import os
from dotenv import load_dotenv

# Setup basic logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

# --- Prompt Template Definition ---
SYSTEM_PROMPT_TEMPLATE = """You are a friendly, precise, and helpful Customer Service assistant for 'Be My Assistant'.
Your primary task is to answer the user's question based *exclusively* on the numbered context chunks provided below.

**Critical Instructions:**
1.  **Use ONLY the Provided Context:** Your answer MUST be derived *solely* from the text within the '[Context n]' chunks. Do NOT use any prior knowledge or external information. It is absolutely critical that you do not add information not present in the context.
2.  **Direct Answers:** Start your answer directly. Do NOT use introductory phrases like "Based on the context...", "According to the document...", etc.
3.  **Answer Formatting:** If the answer involves listing items, features, or steps, use bullet points (*) for clarity. Otherwise, use concise sentences.
4.  **Context Handling:** The relevant information is presented in numbered chunks, like '[Context 1]', '[Context 2]', etc. Synthesize information across chunks if necessary to provide a complete answer, but only use information present within those chunks.
5.  **Language:** If the user's question is in Indonesian, answer in Indonesian. If the question is in English, answer in English.
6.  **If Context is Insufficient:** If the provided context chunks do not contain the information needed to answer the question, respond *only* with: "Maaf, saya tidak dapat menemukan informasi tersebut dalam dokumen yang tersedia." (if the question was Indonesian) or "Sorry, I could not find that information in the available documents." (if the question was English). Do NOT try to guess or apologize further.

---
Context:
{context}
---
"""

# Human prompt template includes the actual user question
HUMAN_PROMPT_TEMPLATE = "Question: {question}"

# Combine into a ChatPromptTemplate
RAG_PROMPT = ChatPromptTemplate.from_messages([
    ("system", SYSTEM_PROMPT_TEMPLATE),
    ("human", HUMAN_PROMPT_TEMPLATE),
])

# --- Helper Function for Formatting Retrieved Documents ---
def format_docs(docs: Optional[List[Tuple[str, float]]]) -> str:
    """
    Formats the retrieved documents into a single string for the prompt context.

    Args:
        docs (Optional[List[Tuple[str, float]]]): The list of (document_text, distance_score) tuples
                                                 from query_vector_store, or None.

    Returns:
        str: A single string containing the formatted document texts, separated by double newlines,
             or a string indicating no context was found if no documents were provided.
    """
    if not docs:
        # Return in Bahasa Indonesia as per system prompt request for final answer language
        return "Tidak ada konteks relevan yang ditemukan dalam dokumen."
    return "\n".join(doc[0] for doc in docs)

# --- Core RAG Orchestration Function ---
def get_rag_response(
    question: str,
    embedding_model: Any, # Expecting an initialized SentenceTransformer model
    vector_collection: Any, # Expecting an initialized Chroma Collection object
) -> Optional[str]:
    """
    Orchestrates the RAG pipeline: retrieves context, builds prompt, calls LLM via llm_interface.

    Args:
        question (str): The user's input question.
        embedding_model (Any): The initialized sentence embedding model.

    Returns:
        Optional[str]: The final answer generated by the LLM based on the context,
                       or None/error message if a critical error occurs.
    """
    logger.info(f"Received RAG request for question: '{question}'")

    if not GOOGLE_API_KEY:
        logger.error("Cannot proceed with RAG: GOOGLE_API_KEY is not configured.")
        return "Error: LLM API Key is not configured." # Return user-facing error

    # --- 1. Retrieve Relevant Documents ---
    logger.debug("Step 1: Querying vector store...")
    retrieved_docs: Optional[List[Tuple[str, float]]] = None # Initialize
    try:
        retrieved_docs = query_vector_store(
            collection=vector_collection,
            query_text=question,
            embedding_model=embedding_model,
            n_results=settings.RAG_NUM_RESULTS
        )
        if retrieved_docs is None:
             logger.warning("Vector store query returned None, assuming no results.")
             retrieved_docs = [] # Treat as empty list
        logger.info(f"Retrieved {len(retrieved_docs)} documents from vector store.")
    except Exception as e:
        logger.error(f"Error querying vector store: {e}", exc_info=True)
        return "Error: Failed to retrieve context information." # Return user-facing error

    # --- 2. Format Retrieved Documents ---
    logger.debug("Step 2: Formatting retrieved documents...")
    context_string = format_docs(retrieved_docs)
    logger.debug(f"Formatted context string (snippet): '{context_string[:200]}...'")

    # --- 3. Build Final Prompt ---
    logger.debug("Step 3: Building final prompt for LLM...")
    try:
        # Format the prompt manually using the template and retrieved context
        final_prompt_string = RAG_PROMPT.format(context=context_string, question=question)
        logger.debug(f"Final prompt ready to be sent to llm_interface (snippet): '...{final_prompt_string[250:]}'")
    except Exception as e:
         logger.error(f"Error formatting final prompt: {e}", exc_info=True)
         return "Error: Failed to build prompt for the language model."

    # --- 4. Call LLM via llm_interface ---
    logger.debug("Step 4: Calling LLM via llm_interface...")
    try:
        # Call the function from llm_interface.py
        final_answer = invoke_llm_langchain(
            prompt=final_prompt_string,
            model_name=settings.LLM_MODEL_NAME,
            temperature=settings.RAG_TEMPERATURE
        )

        # Check if llm_interface returned None (indicating an error there)
        if final_answer is None:
            logger.error("LLM call via llm_interface returned None.")
            return "Error: Failed to get response from the language model (via llm_interface)."

        logger.info("Successfully called LLM via llm_interface and received answer.")
        logger.debug(f"Final Answer (snippet): '{final_answer[:100]}...'")

        return final_answer

    except Exception as e:
        # Catch unexpected errors during the call to invoke_llm_langchain
        logger.error(f"Unexpected error calling invoke_llm_langchain: {e}", exc_info=True)
        return "Error: Failed to generate final answer de to LLM call issue."