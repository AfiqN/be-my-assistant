import logging
from typing import Optional, List, Tuple, Any # Any for model types

# Import functions and classes from other core modules
from .vector_store_manager import (
    query_vector_store
)
# Import LLM function from llm_interface
from .llm_interface import invoke_llm_langchain
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage
from langchain_core.prompts import ChatPromptTemplate
from app.config import settings
from app.schemas import ChatMessage

# Access environment variables (still needed for API Key check)
import os
from dotenv import load_dotenv

# Setup basic logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

# --- Prompt Template Definition ---
SYSTEM_PROMPT_TEMPLATE = """You are 'Be My Assistant', a friendly, helpful, and concise Customer Service AI.
Your goal is to assist users by answering their questions based *only* on the provided 'Retrieved Context' and the ongoing 'Conversation History'.

**Core Instructions:**
1.  **Base Answers on Provided Information:** Answer the user's current question using *only* the information found in the 'Retrieved Context' below or the 'Conversation History'. Do NOT use any external knowledge or make assumptions.
2.  **Fact Source:** The 'Retrieved Context' is the primary source for facts about the company/store. Prioritize it for specific details.
3.  **Conversational Context:** Use the 'Conversation History' (previous `Human:` and `Assistant:` messages) to understand follow-up questions and maintain conversational flow.
4.  **DO NOT Mention Context Source:** Never mention 'Retrieved Context', 'Conversation History', 'documents', or 'context chunks' in your answer to the user. Just provide the answer directly.
5.  **Be Conversational:** Respond naturally. If the user says "hello" or "thank you", respond appropriately (e.g., "Hello! How can I help?", "Sama-sama!" / "You're welcome!"). Don't use the fallback message for simple greetings or closings.
6.  **Clarity and Formatting:** Use clear language. Use bullet points (*) for lists if appropriate based on the context. Ensure the output is clean and ready for display.
7.  **Language:** Respond in the same language as the user's *current* question (Indonesian or English).
8.  **If Information is Unavailable:** If the necessary information to answer the question is genuinely not found in *either* the 'Retrieved Context' or the 'Conversation History', respond *only* with one of the following short phrases:
    * (If user asked in Indonesian): "Maaf, saya belum bisa menjawab pertanyaan tersebut."
    * (If user asked in English): "Sorry, I cannot answer that question right now."
    * Do NOT add any other explanation.

---
Retrieved Context:
{context}
---
"""

# Human prompt template includes the actual user question
HUMAN_PROMPT_TEMPLATE = "Question: {question}"

# Combine into a ChatPromptTemplate
RAG_PROMPT = ChatPromptTemplate.from_messages([
    ("system", SYSTEM_PROMPT_TEMPLATE),
    ("human", HUMAN_PROMPT_TEMPLATE),
])

# --- Helper Function for Formatting Retrieved Documents ---
def format_docs(docs: Optional[List[Tuple[str, float]]]) -> str:
    """
    Formats the retrieved documents into a single string for the prompt context.

    Args:
        docs (Optional[List[Tuple[str, float]]]): The list of (document_text, distance_score) tuples
                                                 from query_vector_store, or None.

    Returns:
        str: A single string containing the formatted document texts, separated by double newlines,
             or a string indicating no context was found if no documents were provided.
    """
    if not docs:
        # Provide a neutral indicator for the LLM, not user-facing
        return "No relevant context was found in the documents."
    return "\n\n---\n\n".join(doc[0] for doc in docs)

# --- Core RAG Orchestration Function ---
def get_rag_response(
    question: str,
    embedding_model: Any, # Expecting an initialized SentenceTransformer model
    vector_collection: Any, # Expecting an initialized Chroma Collection object
    chat_history: Optional[List[ChatMessage]] = None,
) -> Optional[str]:
    """
    Orchestrates the RAG pipeline: retrieves context, builds prompt, calls LLM via llm_interface.

    Args:
        question (str): The user's input question.
        embedding_model (Any): The initialized sentence embedding model.

    Returns:
        Optional[str]: The final answer generated by the LLM based on the context,
                       or None/error message if a critical error occurs.
    """
    logger.info(f"RAG request. Question: '{question}'. History length: {len(chat_history) if chat_history else 0}")

    if not GOOGLE_API_KEY:
        logger.error("Cannot proceed with RAG: GOOGLE_API_KEY is not configured.")
        return "Error: LLM API Key is not configured." # Return user-facing error

    # --- 1. Retrieve Relevant Documents ---
    logger.debug("Step 1: Querying vector store...")
    retrieved_docs: Optional[List[Tuple[str, float]]] = None # Initialize
    try:
        retrieved_docs = query_vector_store(
            collection=vector_collection,
            query_text=question,
            embedding_model=embedding_model,
            n_results=settings.RAG_NUM_RESULTS
        )
        if retrieved_docs is None:
             logger.warning("Vector store query returned None, assuming no results.")
             retrieved_docs = [] # Treat as empty list
        logger.info(f"Retrieved {len(retrieved_docs)} documents from vector store.")
    except Exception as e:
        logger.error(f"Error querying vector store: {e}", exc_info=True)
        return "Error: Failed to retrieve context information." # Return user-facing error

    # --- 2. Format Retrieved Documents ---
    logger.debug("Step 2: Formatting retrieved documents...")
    context_string = format_docs(retrieved_docs)
    logger.debug(f"Formatted context string (snippet): '{context_string[:200]}...'")

    # --- 3. Construct Message List for LLM ---
    logger.debug("Step 3: Constructing message list for LLM...")
    messages: List[BaseMessage] = []
    # Add the main system prompt with instructions and RAG context

    try:
        # Format the prompt manually using the template and retrieved context
        messages.append(SystemMessage(content=SYSTEM_PROMPT_TEMPLATE.format(context=context_string)))
        logger.debug(f"Final prompt ready to be sent to llm_interface (snippet): '...{messages[250:]}'")

        # Add past messages from chat history
        if chat_history:
            for msg in chat_history:
                if msg.role.lower() == 'user':
                    messages.append(HumanMessage(content=msg.content))
                elif msg.role.lower() == 'assistant':
                    messages.append(AIMessage(content=msg.content))
                else: # Ignore other roles if any
                    logger.warning(f"Ignoring message with unknown role in history: {msg.role}")
        # Add the current user question
        messages.append(HumanMessage(content=question))

        logger.debug(f"Constructed {len(messages)} messages for LLM.")
    except Exception as e:
         logger.error(f"Error formatting final prompt: {e}", exc_info=True)
         return "Error: Failed to build prompt for the language model."

    # --- 4. Call LLM via llm_interface ---
    logger.debug("Step 4: Calling LLM via llm_interface...")
    try:
        # Call the function from llm_interface.py
        final_answer = invoke_llm_langchain(
            prompt_input=messages,
            model_name=settings.LLM_MODEL_NAME,
            temperature=settings.RAG_TEMPERATURE
        )

        # Check if llm_interface returned None (indicating an error there)
        if final_answer is None:
            logger.error("LLM call via llm_interface returned None.")
            return "Error: Failed to get response from the language model (via llm_interface)."

        logger.info("Successfully called LLM via llm_interface and received answer.")
        logger.debug(f"Final Answer (snippet): '{final_answer[:100]}...'")

        return final_answer

    except Exception as e:
        # Catch unexpected errors during the call to invoke_llm_langchain
        logger.error(f"Unexpected error calling invoke_llm_langchain: {e}", exc_info=True)
        return "Error: Failed to generate final answer de to LLM call issue."