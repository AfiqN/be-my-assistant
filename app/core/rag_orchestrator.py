import logging
from typing import Optional, List, Tuple, Any # Any for model types

# Import functions and classes from other core modules
from .vector_store_manager import (
    query_vector_store
)
# Import LLM function from llm_interface
from .llm_interface import invoke_llm_langchain
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage
from langchain_core.prompts import ChatPromptTemplate
from app.config import settings
from app.schemas import ChatMessage

# Access environment variables (still needed for API Key check)
import os
from dotenv import load_dotenv

# Setup basic logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

# --- Prompt Template Definition ---
SYSTEM_PROMPT_TEMPLATE = """You are a friendly, precise, and helpful Customer Service assistant for 'Be My Assistant'.
Your primary task is to answer the user's current question based on the conversation history and the retrieved context provided below.

**Critical Instructions:**
1.  **Prioritize Provided Context for Facts:** Use the numbered context chunks ('[Context n]') as the *primary source* for factual information about the company, products, or procedures. Your answer MUST be derived *solely* from this context when answering direct questions about the company. Do NOT use any external knowledge for company-specific facts.
2.  **Use Conversation History for Context:** Refer to the previous turns in the conversation history (`Human:` and `Assistant:`) to understand the flow of the conversation and answer follow-up questions appropriately.
3.  **Synthesize Information:** Combine information from the history and the retrieved context chunks if needed for a comprehensive answer.
4.  **Direct Answers:** Start your answer directly. Avoid phrases like "Based on the context...", "As mentioned before...", etc., unless natural for the flow.
5.  **Answer Formatting:** Use bullet points (*) for lists or steps. Use concise sentences otherwise.
6.  **Language:** Match the user's language (Indonesian or English).
7.  **If Context/History is Insufficient:** If *neither* the retrieved context chunks nor the conversation history contains the information needed to answer the current question, respond *only* with: "Maaf, saya tidak dapat menemukan informasi tersebut dalam dokumen yang tersedia atau riwayat percakapan." (if Indonesian) or "Sorry, I could not find that information in the available documents or conversation history." (if English). Do NOT guess or apologize further.

---
Retrieved Context:
{context}
---
"""

# Human prompt template includes the actual user question
HUMAN_PROMPT_TEMPLATE = "Question: {question}"

# Combine into a ChatPromptTemplate
RAG_PROMPT = ChatPromptTemplate.from_messages([
    ("system", SYSTEM_PROMPT_TEMPLATE),
    ("human", HUMAN_PROMPT_TEMPLATE),
])

# --- Helper Function for Formatting Retrieved Documents ---
def format_docs(docs: Optional[List[Tuple[str, float]]]) -> str:
    """
    Formats the retrieved documents into a single string for the prompt context.

    Args:
        docs (Optional[List[Tuple[str, float]]]): The list of (document_text, distance_score) tuples
                                                 from query_vector_store, or None.

    Returns:
        str: A single string containing the formatted document texts, separated by double newlines,
             or a string indicating no context was found if no documents were provided.
    """
    if not docs:
        # Return in Bahasa Indonesia as per system prompt request for final answer language
        return "Tidak ada konteks relevan yang ditemukan dalam dokumen."
    return "\n".join(doc[0] for doc in docs)

# --- Core RAG Orchestration Function ---
def get_rag_response(
    question: str,
    embedding_model: Any, # Expecting an initialized SentenceTransformer model
    vector_collection: Any, # Expecting an initialized Chroma Collection object
    chat_history: Optional[List[ChatMessage]] = None,
) -> Optional[str]:
    """
    Orchestrates the RAG pipeline: retrieves context, builds prompt, calls LLM via llm_interface.

    Args:
        question (str): The user's input question.
        embedding_model (Any): The initialized sentence embedding model.

    Returns:
        Optional[str]: The final answer generated by the LLM based on the context,
                       or None/error message if a critical error occurs.
    """
    logger.info(f"RAG request. Question: '{question}'. History length: {len(chat_history) if chat_history else 0}")

    if not GOOGLE_API_KEY:
        logger.error("Cannot proceed with RAG: GOOGLE_API_KEY is not configured.")
        return "Error: LLM API Key is not configured." # Return user-facing error

    # --- 1. Retrieve Relevant Documents ---
    logger.debug("Step 1: Querying vector store...")
    retrieved_docs: Optional[List[Tuple[str, float]]] = None # Initialize
    try:
        retrieved_docs = query_vector_store(
            collection=vector_collection,
            query_text=question,
            embedding_model=embedding_model,
            n_results=settings.RAG_NUM_RESULTS
        )
        if retrieved_docs is None:
             logger.warning("Vector store query returned None, assuming no results.")
             retrieved_docs = [] # Treat as empty list
        logger.info(f"Retrieved {len(retrieved_docs)} documents from vector store.")
    except Exception as e:
        logger.error(f"Error querying vector store: {e}", exc_info=True)
        return "Error: Failed to retrieve context information." # Return user-facing error

    # --- 2. Format Retrieved Documents ---
    logger.debug("Step 2: Formatting retrieved documents...")
    context_string = format_docs(retrieved_docs)
    logger.debug(f"Formatted context string (snippet): '{context_string[:200]}...'")

    # --- 3. Construct Message List for LLM ---
    logger.debug("Step 3: Constructing message list for LLM...")
    messages: List[BaseMessage] = []
    # Add the main system prompt with instructions and RAG context

    try:
        # Format the prompt manually using the template and retrieved context
        messages.append(SystemMessage(content=SYSTEM_PROMPT_TEMPLATE.format(context=context_string)))
        logger.debug(f"Final prompt ready to be sent to llm_interface (snippet): '...{messages[250:]}'")

        # Add past messages from chat history
        if chat_history:
            for msg in chat_history:
                if msg.role.lower() == 'user':
                    messages.append(HumanMessage(content=msg.content))
                elif msg.role.lower() == 'assistant':
                    messages.append(AIMessage(content=msg.content))
                else: # Ignore other roles if any
                    logger.warning(f"Ignoring message with unknown role in history: {msg.role}")
        # Add the current user question
        messages.append(HumanMessage(content=question))

        logger.debug(f"Constructed {len(messages)} messages for LLM.")
    except Exception as e:
         logger.error(f"Error formatting final prompt: {e}", exc_info=True)
         return "Error: Failed to build prompt for the language model."

    # --- 4. Call LLM via llm_interface ---
    logger.debug("Step 4: Calling LLM via llm_interface...")
    try:
        # Call the function from llm_interface.py
        final_answer = invoke_llm_langchain(
            prompt_input=messages,
            model_name=settings.LLM_MODEL_NAME,
            temperature=settings.RAG_TEMPERATURE
        )

        # Check if llm_interface returned None (indicating an error there)
        if final_answer is None:
            logger.error("LLM call via llm_interface returned None.")
            return "Error: Failed to get response from the language model (via llm_interface)."

        logger.info("Successfully called LLM via llm_interface and received answer.")
        logger.debug(f"Final Answer (snippet): '{final_answer[:100]}...'")

        return final_answer

    except Exception as e:
        # Catch unexpected errors during the call to invoke_llm_langchain
        logger.error(f"Unexpected error calling invoke_llm_langchain: {e}", exc_info=True)
        return "Error: Failed to generate final answer de to LLM call issue."